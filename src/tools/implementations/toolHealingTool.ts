/*---------------------------------------------------------------------------------------------
 * Tool Healing Tool - CLI interface for the healing system management
 *--------------------------------------------------------------------------------------------*/

import { BaseTool } from '../base/baseTool';
import { 
    CliToolInvocationOptions,
    CliCancellationToken,
    CliToolResult,
    // CliTextPart, // Unused import
    // CliErrorPart // Unused import
} from '../types/cliTypes';
import { 
    HealingIntegration, 
    HealingConfig 
} from '../healing/healingIntegration';
// import { ToolHealer } from '../healing/toolHealer'; // Unused import
import { LLMService } from '../healing/llmService';
import { DEFAULT_API_CONFIG, AVAILABLE_MODELS } from '../healing/config/apiConfig';

interface IToolHealingParams {
    action: 'heal' | 'report' | 'health' | 'patterns' | 'clear_cache' | 'config' | 'test' | 'api_test' | 'api_status';
    source_model?: string;
    file_path?: string;
    old_string?: string;
    new_string?: string;
    error_message?: string;
    model_family?: string;
    api_key?: string;
    config?: {
        enable_healing?: boolean;
        enable_metacognition?: boolean;
        max_healing_attempts?: number;
        healing_timeout?: number;
        log_healing?: boolean;
        cache_healings?: boolean;
        metacognition_model?: string;
        openrouter_api_key?: string;
        site_url?: string;
        site_name?: string;
    };
    [key: string]: unknown;
}

export class ToolHealingTool extends BaseTool<IToolHealingParams> {
    readonly name = 'tool_healing';
    readonly description = `Advanced tool healing system for auto-correcting LLM-generated parameters.

Automatically fixes common issues in tool parameters generated by different LLM models (GPT-4, Claude, Gemini, DeepSeek) through:

- **Phase 1: Unescape Healing** - Fix over-escaping issues (especially Gemini)
- **Phase 2: Pattern Healing** - Apply model-specific correction patterns  
- **Phase 3: LLM Metacognition** - Use GPT-4o-mini for intelligent parameter correction
- **Phase 4: NewString Adjustment** - Maintain consistency between old/new strings

Features: Intelligent caching, detailed metrics, health monitoring, model-specific bug patterns, and comprehensive reporting.

Use when: Tool parameters fail to match content, debugging parameter issues, analyzing healing performance, or managing healing configuration.`;

    readonly tags = ['healing', 'correction', 'llm', 'metacognition', 'parameters'];
    readonly category = 'development';
    readonly complexity: 'advanced' = 'advanced';

    readonly inputSchema = {
        type: 'object',
        properties: {
            action: {
                type: 'string',
                enum: ['heal', 'report', 'health', 'patterns', 'clear_cache', 'config', 'test', 'api_test', 'api_status'],
                description: 'Action to perform with the healing system'
            },
            source_model: {
                type: 'string',
                description: 'Source LLM model that generated the problematic parameters',
                examples: ['gpt-4', 'claude-3-sonnet', 'gemini-pro', 'deepseek-coder']
            },
            file_path: {
                type: 'string',
                description: 'Path to the file where parameter matching failed'
            },
            old_string: {
                type: 'string',
                description: 'The problematic old_string parameter that failed to match'
            },
            new_string: {
                type: 'string',
                description: 'The corresponding new_string parameter'
            },
            error_message: {
                type: 'string',
                description: 'The error message from the failed tool execution'
            },
            model_family: {
                type: 'string',
                description: 'Model family to analyze patterns for (gemini, claude, deepseek, gpt)',
                examples: ['gemini', 'claude', 'deepseek', 'gpt']
            },
            api_key: {
                type: 'string',
                description: 'OpenRouter API key for LLM metacognition (can also use OPENROUTER_API_KEY env var)'
            },
            config: {
                type: 'object',
                properties: {
                    enable_healing: {
                        type: 'boolean',
                        description: 'Enable/disable the healing system',
                        default: true
                    },
                    enable_metacognition: {
                        type: 'boolean',
                        description: 'Enable/disable LLM metacognition healing',
                        default: true
                    },
                    max_healing_attempts: {
                        type: 'number',
                        description: 'Maximum number of healing attempts per failure',
                        default: 3,
                        minimum: 1,
                        maximum: 10
                    },
                    healing_timeout: {
                        type: 'number',
                        description: 'Timeout for healing operations in milliseconds',
                        default: 30000,
                        minimum: 5000,
                        maximum: 300000
                    },
                    log_healing: {
                        type: 'boolean',
                        description: 'Enable detailed healing logging',
                        default: true
                    },
                    cache_healings: {
                        type: 'boolean',
                        description: 'Enable caching of successful healings',
                        default: true
                    },
                    metacognition_model: {
                        type: 'string',
                        description: 'LLM model to use for metacognitive analysis',
                        default: 'gpt-4o-mini',
                        examples: ['gpt-4o-mini', 'gpt-4', 'claude-3-haiku']
                    },
                    openrouter_api_key: {
                        type: 'string',
                        description: 'OpenRouter API key for real LLM integration'
                    },
                    site_url: {
                        type: 'string',
                        description: 'Site URL for OpenRouter rankings (optional)'
                    },
                    site_name: {
                        type: 'string',
                        description: 'Site name for OpenRouter rankings (optional)'
                    }
                },
                description: 'Healing system configuration options'
            }
        },
        required: ['action']
    };

    private integration: HealingIntegration;

    constructor() {
        super();
        this.integration = new HealingIntegration();
        this.integration.setLogger((level, message, data) => {
            if (level === 'error') {
                console.error(`[Healing] ${message}`, data ? JSON.stringify(data, null, 2) : '');
            } else if (level === 'info') {
                console.log(`[Healing] ${message}`, data ? JSON.stringify(data, null, 2) : '');
            } else {
                console.debug(`[Healing] ${message}`, data ? JSON.stringify(data, null, 2) : '');
            }
        });
    }

    async invoke(
        options: CliToolInvocationOptions<IToolHealingParams>,
        _token: CliCancellationToken
    ): Promise<CliToolResult> {

        const { action, source_model, file_path, old_string, new_string, error_message, model_family, api_key, config } = options.input;

        try {
            // Apply configuration if provided
            if (config) {
                this.integration = new HealingIntegration(config as HealingConfig);
            }

            switch (action) {
                case 'heal':
                    return await this.handleHeal(source_model, file_path, old_string, new_string, error_message);
                
                case 'report':
                    return await this.handleReport();
                
                case 'health':
                    return await this.handleHealth();
                
                case 'patterns':
                    return await this.handlePatterns(model_family);
                
                case 'clear_cache':
                    return await this.handleClearCache();
                
                case 'config':
                    return await this.handleConfig();
                
                case 'test':
                    return await this.handleTest();
                
                case 'api_test':
                    return await this.handleApiTest(api_key, config);
                
                case 'api_status':
                    return await this.handleApiStatus();
                
                default:
                    return this.createErrorResult(`Unknown action: ${action}`);
            }

        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            return this.createErrorResult(`Tool healing operation failed: ${errorMessage}`);
        }
    }

    private async handleHeal(
        sourceModel?: string,
        filePath?: string,
        oldString?: string,
        newString?: string,
        errorMessage?: string
    ): Promise<CliToolResult> {

        if (!sourceModel || !oldString || !newString || !errorMessage) {
            return this.createErrorResult(
                'heal action requires: source_model, old_string, new_string, and error_message'
            );
        }

        // Create mock tool and error for healing test
        const mockTool = {
            name: 'test_healing',
            description: 'Test healing operation',
            category: 'test',
            complexity: 'basic' as const,
            inputSchema: {},
            tags: ['test']
        };

        const mockParameters = {
            oldString,
            newString,
            filePath: filePath || ''
        };

        const mockError = new Error(errorMessage) as Error & { type: string; filePath?: string };
        mockError.type = 'NoMatchError';
        if (filePath) {
            mockError.filePath = filePath;
        }

        // Read file content if path provided
        // let _fileContent = '';
        if (filePath && this.context?.fileSystem) {
            try {
                // _fileContent = await this.context.fileSystem.readFile(filePath);
                await this.context.fileSystem.readFile(filePath);
            } catch {
                console.warn(`Could not read file: ${filePath}`);
            }
        }

        const startTime = performance.now();
        const healingResult = await this.integration.attemptHealing(
            sourceModel,
            mockTool as any,
            mockParameters,
            mockError as any,
            this.context
        );
        const duration = performance.now() - startTime;

        const report = {
            source_model: sourceModel,
            file_path: filePath,
            original_parameters: {
                old_string: oldString,
                new_string: newString
            },
            healing_result: {
                success: healingResult.success,
                healing_method: healingResult.healingMethod,
                confidence: healingResult.confidence || 0,
                metacognition_used: healingResult.metacognitionUsed,
                healing_attempts: healingResult.healingAttempts,
                healing_time_ms: Math.round(healingResult.healingTime),
                healed_parameters: healingResult.healedParameters
            },
            execution_time_ms: Math.round(duration)
        };

        const status = healingResult.success 
            ? `✅ Healing successful using ${healingResult.healingMethod} (confidence: ${((healingResult.confidence || 0) * 100).toFixed(1)}%)`
            : `❌ Healing failed after ${healingResult.healingAttempts} attempts`;

        return this.createSuccessResult(report, status);
    }

    private async handleReport(): Promise<CliToolResult> {
        const report = this.integration.generateHealingReport();
        // const _metrics = this.integration.getMetrics(); // Available for future reporting

        const summary = `📊 **Tool Healing System Report**

🔢 **Summary Statistics:**
- Total Healing Attempts: ${report.summary.totalHealings}
- Success Rate: ${(report.summary.successRate * 100).toFixed(1)}%
- Average Healing Time: ${report.summary.averageTime.toFixed(1)}ms
- Metacognition Usage: ${(report.summary.metacognitionUsage * 100).toFixed(1)}%

📈 **Model Performance:**
${Object.entries(report.modelBreakdown).map(([model, stats]) => 
    `- **${model.toUpperCase()}**: ${stats.attempts} attempts, ${stats.successes} successes (${((stats.successes / stats.attempts) * 100).toFixed(1)}%)`
).join('\n')}

🔧 **Top Healing Methods:**
${report.topHealingMethods.slice(0, 3).map((method, i) => 
    `${i + 1}. ${method.method}: ${method.count} uses (${(method.successRate * 100).toFixed(1)}% success)`
).join('\n')}

⚡ **Performance Metrics:**
- Cache Hit Rate: ${(report.performance.cacheHitRate * 100).toFixed(1)}%
- Cache Entries: ${report.performance.totalCacheEntries}
- Average Healing Time: ${report.performance.averageHealingTime.toFixed(1)}ms`;

        return this.createSuccessResult(report, summary);
    }

    private async handleHealth(): Promise<CliToolResult> {
        const health = this.integration.healthCheck();
        
        const statusEmoji = {
            'healthy': '✅',
            'warning': '⚠️',
            'error': '❌'
        };

        const message = `${statusEmoji[health.status]} **Healing System Health: ${health.status.toUpperCase()}**

${health.issues.length > 0 ? `🚨 **Issues Detected:**\n${health.issues.map(issue => `- ${issue}`).join('\n')}\n` : ''}

📊 **Current Metrics:**
- Total Attempts: ${health.metrics.totalAttempts}
- Success Rate: ${health.metrics.totalAttempts > 0 ? ((health.metrics.successfulHealings / health.metrics.totalAttempts) * 100).toFixed(1) : 0}%
- Cache Hit Rate: ${(health.metrics.cacheHitRate * 100).toFixed(1)}%
- Average Healing Time: ${health.metrics.averageHealingTime.toFixed(1)}ms
- Metacognition Usage: ${(health.metrics.metacognitionUsageRate * 100).toFixed(1)}%

⚙️ **Configuration:**
- Healing Enabled: ${health.config.enableHealing ? '✅' : '❌'}
- Metacognition Enabled: ${health.config.enableMetacognition ? '✅' : '❌'}
- Max Attempts: ${health.config.maxHealingAttempts}
- Timeout: ${health.config.healingTimeout}ms
- Caching: ${health.config.cacheHealings ? '✅' : '❌'}`;

        return this.createSuccessResult(health, message);
    }

    private async handlePatterns(modelFamily?: string): Promise<CliToolResult> {
        if (!modelFamily) {
            const allPatterns = {
                'gemini': this.integration.getModelBugPatterns('gemini'),
                'claude': this.integration.getModelBugPatterns('claude'),
                'deepseek': this.integration.getModelBugPatterns('deepseek'),
                'gpt': this.integration.getModelBugPatterns('gpt')
            };

            const message = `🔍 **Model Bug Patterns Overview:**

${Object.entries(allPatterns).map(([family, patterns]) => 
    `**${family.toUpperCase()}** (${patterns.length} patterns):\n${patterns.map(p => 
        `- ${p.name}: ${p.pattern.source} (confidence: ${(p.confidence * 100).toFixed(0)}%)`
    ).join('\n')}`
).join('\n\n')}

**Usage Examples:**
\`tool_healing patterns --model_family "gemini"\` - Show Gemini-specific patterns
\`tool_healing heal --source_model "gemini-pro" --old_string "text\\\\nwith\\\\tescapes" --new_string "updated" --error_message "No match found"\` - Test healing`;

            return this.createSuccessResult(allPatterns, message);
        }

        const patterns = this.integration.getModelBugPatterns(modelFamily);
        const message = `🔍 **${modelFamily.toUpperCase()} Bug Patterns:**

${patterns.length > 0 
    ? patterns.map((pattern, i) => 
        `${i + 1}. **${pattern.name}**\n   Pattern: \`${pattern.pattern.source}\`\n   Confidence: ${(pattern.confidence * 100).toFixed(0)}%`
    ).join('\n\n')
    : 'No specific patterns documented for this model family.'
}

**Testing Pattern:**
\`tool_healing heal --source_model "${modelFamily}" --old_string "problematic_text" --new_string "fixed_text" --error_message "No match found"\``;

        return this.createSuccessResult({ model_family: modelFamily, patterns }, message);
    }

    private async handleClearCache(): Promise<CliToolResult> {
        const stats = this.integration.getCacheStats();
        this.integration.clearCache();
        
        const message = `🧹 **Healing Cache Cleared Successfully**

**Previous Cache Statistics:**
- Size: ${stats.size} entries
- Hits: ${stats.hits}
- Misses: ${stats.misses}
- Hit Rate: ${(stats.hitRate * 100).toFixed(1)}%

Cache has been reset and will rebuild on next healing operation.`;

        return this.createSuccessResult({ cleared_stats: stats }, message);
    }

    private async handleConfig(): Promise<CliToolResult> {
        const health = this.integration.healthCheck();
        const config = health.config;
        
        const message = `⚙️ **Current Healing System Configuration:**

**Core Settings:**
- Healing Enabled: ${config.enableHealing ? '✅ Yes' : '❌ No'}
- Metacognition Enabled: ${config.enableMetacognition ? '✅ Yes' : '❌ No'}
- Max Healing Attempts: ${config.maxHealingAttempts}
- Healing Timeout: ${config.healingTimeout}ms

**Performance Settings:**
- Healing Logging: ${config.logHealing ? '✅ Enabled' : '❌ Disabled'}
- Cache Healings: ${config.cacheHealings ? '✅ Enabled' : '❌ Disabled'}
- Metacognition Model: ${config.metacognitionModel}

**To Update Configuration:**
\`tool_healing config --config '{"enable_healing": true, "max_healing_attempts": 5}'\`

**Available Models for Metacognition:**
- gpt-4o-mini (recommended, fast and cost-effective)
- gpt-4 (highest quality, slower)
- claude-3-haiku (alternative option)`;

        return this.createSuccessResult({ current_config: config }, message);
    }

    private async handleTest(): Promise<CliToolResult> {
        const testCases = [
            {
                name: 'Gemini Over-escaping',
                sourceModel: 'gemini-pro',
                oldString: 'Hello\\nWorld\\tTest',
                newString: 'Updated\\nWorld\\tTest',
                expectedMethod: 'unescape'
            },
            {
                name: 'Claude Whitespace',
                sourceModel: 'claude-3-sonnet',
                oldString: 'Text  with   extra    spaces',
                newString: 'Updated  text',
                expectedMethod: 'unescape'
            },
            {
                name: 'DeepSeek JSON Escaping',
                sourceModel: 'deepseek-coder',
                oldString: 'Text with \\"quotes\\"',
                newString: 'Updated \\"quotes\\"',
                expectedMethod: 'unescape'
            },
            {
                name: 'Generic Trailing Spaces',
                sourceModel: 'gpt-4',
                oldString: 'Text with trailing spaces   ',
                newString: 'Updated text   ',
                expectedMethod: 'pattern_matching'
            }
        ];

        const results = [];
        let passedTests = 0;

        for (const testCase of testCases) {
            const mockTool = {
                name: 'test_healing',
                description: 'Test case',
                category: 'test',
                complexity: 'basic' as const,
                inputSchema: {},
                tags: ['test']
            };

            const mockError = new Error('Test no match error') as Error & { type: string };
            mockError.type = 'NoMatchError';

            try {
                const result = await this.integration.attemptHealing(
                    testCase.sourceModel,
                    mockTool as any,
                    {
                        oldString: testCase.oldString,
                        newString: testCase.newString
                    },
                    mockError as any
                );

                const passed = result.success && result.healingMethod === testCase.expectedMethod;
                if (passed) {passedTests++;}

                results.push({
                    name: testCase.name,
                    model: testCase.sourceModel,
                    success: result.success,
                    method: result.healingMethod || 'none',
                    expected: testCase.expectedMethod,
                    passed,
                    confidence: result.confidence || 0,
                    time: result.healingTime
                });

            } catch (error) {
                results.push({
                    name: testCase.name,
                    model: testCase.sourceModel,
                    success: false,
                    method: 'error',
                    expected: testCase.expectedMethod,
                    passed: false,
                    error: error instanceof Error ? error.message : String(error)
                });
            }
        }

        const message = `🧪 **Healing System Test Results**

**Overall: ${passedTests}/${testCases.length} tests passed** ${passedTests === testCases.length ? '✅' : '⚠️'}

${results.map(result => 
    `**${result.name}** (${result.model})
${result.passed ? '✅' : '❌'} Success: ${result.success}, Method: ${result.method} (expected: ${result.expected})
${result.confidence ? `Confidence: ${(result.confidence * 100).toFixed(1)}%, ` : ''}${result.time ? `Time: ${Math.round(result.time)}ms` : ''}${result.error ? `Error: ${result.error}` : ''}`
).join('\n\n')}

${passedTests < testCases.length ? 
    `⚠️ **${testCases.length - passedTests} test(s) failed** - Check healing system configuration and patterns.` : 
    '✅ **All tests passed** - Healing system is working correctly!'
}`;

        return this.createSuccessResult({ 
            test_results: results, 
            passed: passedTests, 
            total: testCases.length,
            success_rate: passedTests / testCases.length
        }, message);
    }

    private async handleApiTest(apiKey?: string, config?: unknown): Promise<CliToolResult> {
        try {
            const apiConfig = {
                ...DEFAULT_API_CONFIG,
                openRouterApiKey: apiKey || (config as any)?.openrouter_api_key || DEFAULT_API_CONFIG.openRouterApiKey,
                siteUrl: (config as any)?.site_url || DEFAULT_API_CONFIG.siteUrl,
                siteName: (config as any)?.site_name || DEFAULT_API_CONFIG.siteName
            };

            if (!apiConfig.openRouterApiKey) {
                return this.createErrorResult(
                    'API key is required. Provide via --api_key parameter, config.openrouter_api_key, or OPENROUTER_API_KEY environment variable.'
                );
            }

            const llmService = new LLMService(apiConfig);
            llmService.setLogger((level, message, data) => {
                console.log(`[${level.toUpperCase()}] ${message}`, data ? JSON.stringify(data, null, 2) : '');
            });

            const startTime = performance.now();
            const testResult = await llmService.testConnection();
            const totalTime = performance.now() - startTime;

            if (testResult.success) {
                const message = `✅ **API Connection Test Successful**

**Connection Details:**
- Model: ${testResult.model || 'unknown'}
- Response Time: ${Math.round(testResult.responseTime || 0)}ms
- Total Test Time: ${Math.round(totalTime)}ms
- API Endpoint: https://openrouter.ai/api/v1
- Status: Connected and responsive

**Next Steps:**
- Use \`tool_healing api_status\` to view available models and pricing
- Test real healing with \`tool_healing heal\` action
- Configure healing system with \`tool_healing config\``;

                return this.createSuccessResult({
                    success: true,
                    model: testResult.model,
                    responseTime: testResult.responseTime,
                    totalTime: Math.round(totalTime),
                    apiEndpoint: 'https://openrouter.ai/api/v1'
                }, message);
            } else {
                const message = `❌ **API Connection Test Failed**

**Error Details:**
- Error: ${testResult.error}
- Total Test Time: ${Math.round(totalTime)}ms
- API Endpoint: https://openrouter.ai/api/v1

**Troubleshooting:**
1. Check your API key is valid and active
2. Verify network connectivity to openrouter.ai
3. Ensure you have sufficient credits/quota
4. Try again in a few moments (rate limiting)

**Common Solutions:**
- Set OPENROUTER_API_KEY environment variable
- Use --api_key parameter with valid key
- Check OpenRouter dashboard for account status`;

                return this.createErrorResult(message);
            }

        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            return this.createErrorResult(`API test failed: ${errorMessage}`);
        }
    }

    private async handleApiStatus(): Promise<CliToolResult> {
        const models = Object.entries(AVAILABLE_MODELS).map(([key, model]) => ({
            key,
            name: model.displayName,
            openRouterName: model.openRouterName,
            contextWindow: model.contextWindow,
            maxTokens: model.maxTokens,
            costPer1kTokens: model.costPer1kTokens,
            recommended: key === 'gpt-4o-mini'
        }));

        const recommendedModel = models.find(m => m.recommended);
        const cheapestModel = models.reduce((min, model) => 
            model.costPer1kTokens.prompt < min.costPer1kTokens.prompt ? model : min
        );
        const fastestModel = models.find(m => m.key === 'gpt-4o-mini') || models[0];

        const message = `🤖 **LLM API Status & Available Models**

**Current Configuration:**
- Default Model: ${DEFAULT_API_CONFIG.defaultModel}
- API Endpoint: https://openrouter.ai/api/v1
- Timeout: ${DEFAULT_API_CONFIG.timeout}ms
- Max Retries: ${DEFAULT_API_CONFIG.maxRetries}

**Available Models for Metacognition:**

${models.map(model => 
    `**${model.name}** ${model.recommended ? '⭐ (Recommended)' : ''}
- OpenRouter: ${model.openRouterName}
- Context: ${model.contextWindow.toLocaleString()} tokens
- Max Output: ${model.maxTokens.toLocaleString()} tokens
- Cost: $${model.costPer1kTokens.prompt}/1k prompt + $${model.costPer1kTokens.completion}/1k completion`
).join('\n\n')}

**Model Recommendations:**
- **For Cost-Effectiveness**: ${recommendedModel?.name} (${recommendedModel?.openRouterName})
- **For Cheapest Option**: ${cheapestModel.name} (${cheapestModel.openRouterName})
- **For Speed**: ${fastestModel.name} (${fastestModel.openRouterName})

**API Key Setup:**
1. Get your API key from: https://openrouter.ai/
2. Set environment variable: \`export OPENROUTER_API_KEY="your-key-here"\`
3. Or use --api_key parameter in commands
4. Test connection: \`tool_healing api_test\`

**Usage Estimates (per healing attempt):**
- Prompt: ~300-500 tokens
- Response: ~100-200 tokens  
- Estimated cost per healing: $0.0001 - $0.001 (depending on model)`;

        return this.createSuccessResult({
            availableModels: models,
            defaultModel: DEFAULT_API_CONFIG.defaultModel,
            apiEndpoint: 'https://openrouter.ai/api/v1',
            recommendations: {
                costEffective: recommendedModel?.key,
                cheapest: cheapestModel.key,
                fastest: fastestModel.key
            }
        }, message);
    }
}